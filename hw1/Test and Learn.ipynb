{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import load_policy\n",
    "import tf_util\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the model to work with\n",
    "model = \"Humanoid-v2\"\n",
    "logfile = \"logfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the experiment data that was generated by the expert policy\n",
    "loaded = pickle.load( open( \"/home/yousof/courses/cs294_DeepRL/hw1/expert_data/\"+model+\".pkl\", \"rb\" ) )\n",
    "print(type(loaded) )\n",
    "for key in loaded.keys():\n",
    "    print(key)\n",
    "print (type (loaded[\"actions\"]))\n",
    "print (loaded[\"actions\"].shape)\n",
    "print (loaded[\"observations\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example in Humanoid-v2 with 20 roll outs we have\n",
    "20 sets of training data. Each ahave 17 actions and 376 observations.\n",
    "The would means the input of the model would be 376 and the output would be 17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gym enviornment\n",
    "env = gym.make(model)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the observation space dimentionality of the enviornment\n",
    "spc = env.observation_space\n",
    "print(type(spc.shape))\n",
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the data, including:\n",
    "    normalization and save the normalization parameters\n",
    "\"\"\"\n",
    "\n",
    "num_rollout = 20\n",
    "\n",
    "x_i = loaded[\"observations\"]\n",
    "y_i = np.squeeze(loaded[\"actions\"],axis=1)\n",
    "num_examples = x_i.shape[0]\n",
    "\n",
    "num_ins = x_i.shape[1]\n",
    "num_outs = y_i.shape[1]\n",
    "\n",
    "print (\"X RAW shape:\", x_i.shape, \"Y RAW shape:\",y_i.shape)\n",
    "\n",
    "\n",
    "bat = num_examples / num_rollout\n",
    "# print(bat)\n",
    "bat = int(bat)\n",
    "\n",
    "x = np.array([], dtype=np.float16).reshape(0,num_ins * 2 )\n",
    "y = np.array([], dtype=np.float16).reshape(0,num_outs)\n",
    "\n",
    "\n",
    "for idx, i in enumerate(range(0,num_examples,bat)):\n",
    "    start = i\n",
    "    end = i+bat\n",
    "    xpreK = x_i[start:end-1,:]\n",
    "    xK = x_i[start+1:end,:]\n",
    "    x = np.concatenate( (x,np.concatenate((xK, xK-xpreK ) , axis = 1) ), axis = 0)\n",
    "    y = np.concatenate( (y , y_i[start+1:end,:]), axis = 0  )\n",
    "\n",
    "\n",
    "print (\"After Adding \\\"obs_k - obs_(k-1)\\\". X shape:\", x.shape, \"Y shape:\",y.shape)\n",
    "\n",
    "num_examples = x.shape[0]\n",
    "\n",
    "\n",
    "# normalize the values\n",
    "def normalize(x,mean=None,std=None):\n",
    "    if (mean is None):\n",
    "        mean = np.mean(x,axis=0)\n",
    "        std = np.std(x,axis=0)\n",
    "        normalized = (x-mean)/std\n",
    "        return normalized, mean, std\n",
    "    else:\n",
    "        return (x-mean)/std\n",
    "\n",
    "def unnormalize(x, mean, std):\n",
    "    unnorm = x * std + mean\n",
    "    return unnorm\n",
    "\n",
    "x_norm, x_mean, x_std = normalize(x)\n",
    "y_norm, y_mean, y_std = normalize(y)\n",
    "\n",
    "print (\"shape of the normlization results for x_norm {}, x_mean {} and x_std {}\".format(\n",
    "    x_norm.shape, x_mean.shape, x_std.shape))\n",
    "print (\"shape of the normlization results for y_norm {}, y_mean {} and y_std {}\".format(\n",
    "    y_norm.shape, y_mean.shape, y_std.shape))\n",
    "\n",
    "# remove the nans from the normalized vector\n",
    "x_mask = (x_std!=0)\n",
    "y_mask = (y_std!=0)\n",
    "t = np.tile(x_mask,(num_examples,1))\n",
    "x_norm = x_norm[t].reshape([num_examples,-1])\n",
    "t = np.tile(y_mask,(num_examples,1))\n",
    "y_norm = y_norm[t].reshape([num_examples,-1])\n",
    "print (\"The shapes after removing the dimenstions with **std = 0**, x_norm shape {}, y_norm shape {}\".format(\n",
    "    x_norm.shape, y_norm.shape))\n",
    "\n",
    "y_un = unnormalize(y_norm,y_mean,y_std)\n",
    "print(\"error after undoing the normalization\" ,np.sum(np.abs(y_un - y)))\n",
    "\n",
    "print (\"Is there any Nan in training date?\" ,np.isnan(x_norm).any(),np.isnan(y_norm).any(),\n",
    "       np.isinf(x_norm).any(),np.isinf(y_norm).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the normalization paramters\n",
    "toSave = dict()\n",
    "toSave[\"x_mean\"] = x_mean\n",
    "toSave[\"y_mean\"] = y_mean\n",
    "toSave[\"x_std\"] = x_std\n",
    "toSave[\"y_std\"] = y_std\n",
    "pickle.dump(toSave,open( \"/home/yousof/courses/cs294_DeepRL/hw1/expert_data/\"+model+\"_normalization.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a dense network to learn frome the training data as easy as possible\n",
    "\n",
    "model_in_size = x_norm.shape[1]\n",
    "model_out_size = y_norm.shape[1]\n",
    "\n",
    "x_place = tf.placeholder(tf.float16,shape=(None, model_in_size),name=\"input\")\n",
    "y_place = tf.placeholder(tf.float16,shape=(None, model_out_size),name=\"label\")\n",
    "\n",
    "mean_init = 0\n",
    "std_init = 0.001\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "\n",
    "dens = tf.layers.dense(x_place,512,\n",
    "                       activation=tf.nn.tanh,\n",
    "                       kernel_initializer=tf.truncated_normal_initializer(stddev=std_init),\n",
    "                       name = \"dense1\",\n",
    "                       kernel_regularizer=regularizer)\n",
    "# dens = tf.layers.dense(dens,1024,\n",
    "#                        activation=tf.nn.sigmoid,\n",
    "#                        kernel_initializer=tf.truncated_normal_initializer(stddev=std_init),\n",
    "#                        name = \"dense2\",\n",
    "#                        kernel_regularizer=regularizer)\n",
    "# dens = tf.layers.dense(dens,128,\n",
    "#                        activation=tf.nn.tanh,\n",
    "#                        kernel_initializer=tf.truncated_normal_initializer(stddev=std_init),\n",
    "#                        name = \"dense3\",\n",
    "#                        kernel_regularizer=regularizer)\n",
    "# dens = tf.layers.dense(dens,128,\n",
    "#                        activation=tf.nn.tanh,\n",
    "#                        kernel_initializer=tf.truncated_normal_initializer(stddev=std_init),\n",
    "#                        name = \"dense4\",\n",
    "#                        kernel_regularizer=regularizer)\n",
    "pred = tf.layers.dense(dens,model_out_size,\n",
    "                       activation=tf.nn.tanh,\n",
    "                       kernel_initializer=tf.truncated_normal_initializer(stddev=std_init),\n",
    "                       name = \"pred\",\n",
    "                       kernel_regularizer=regularizer)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_place , pred)\n",
    "optmizer_op = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "train_op = optmizer_op.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size,x,y):\n",
    "    x_size = x.shape[1]\n",
    "    y_size = y.shape[1]\n",
    "    total_number = x.shape[0]\n",
    "    xy = np.concatenate((x,y),axis=1)\n",
    "    np.random.shuffle(xy)\n",
    "    for batch_i in range(0,total_number,batch_size):\n",
    "        start = batch_i\n",
    "        end = batch_i+ batch_size\n",
    "        end = min (end, total_number)\n",
    "        x_out = xy[start:end,0:x_size]\n",
    "        y_out = xy[start:end,x_size:]\n",
    "        #print(x_out.shape,y_out.shape)\n",
    "        yield x_out,y_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 5000\n",
    "batch_size = 1024*16\n",
    "\n",
    "#merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logfile, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epoch):\n",
    "#         i_batch = 0\n",
    "#         for batch_x, batch_y in get_batches(batch_size,x_norm,y_norm):\n",
    "#             i_batch += 1\n",
    "#             out = sess.run([loss ,train_op], feed_dict = {x_place: batch_x , y_place: batch_y})\n",
    "#             print(\"Epoch: {} batch: {} loss: {}\"\n",
    "#                   .format(epoch,i_batch,out[0]))\n",
    "        out = sess.run([loss ,train_op], feed_dict = {x_place: x_norm , y_place: y_norm})\n",
    "        print(\"Epoch: {} loss: {}\"\n",
    "              .format(epoch,out[0]))\n",
    "#         print(\"===================================================\")\n",
    "    save_path = saver.save(sess, \"models/\"+model+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model_path = \"models/\" + model\n",
    "render = True\n",
    "\n",
    "\n",
    "def prepareInput(obs, preObs, size):\n",
    "    obs =obs.reshape([1,size])\n",
    "    preObs = preObs.reshape([1,size])\n",
    "    model_in = np.concatenate( (obs,obs-preObs), axis = 1 )\n",
    "    model_in = normalize(model_in,mean = x_mean, std = x_std)\n",
    "    model_in = model_in[x_mask.reshape([1,size*2])]\n",
    "    model_in = np.expand_dims(model_in, axis=0)\n",
    "    return model_in\n",
    "\n",
    "def prepareOutput(modelOut):\n",
    "    return unnormalize(modelOut,y_mean,y_std)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # load the model\n",
    "    saver = tf.train.import_meta_graph(model_path + '.ckpt.meta')\n",
    "    saver.restore(sess,model_path+\".ckpt\")\n",
    "    graph = tf.get_default_graph()\n",
    "    policy_out = graph.get_tensor_by_name('pred/Tanh:0')#I_want_to_train_only_these/fc8/BiasAdd:0\n",
    "    x_place = graph.get_tensor_by_name('input:0')\n",
    "    \n",
    "    def run_policy(policy_in):\n",
    "        return sess.run(policy_out, feed_dict = {x_place: policy_in})\n",
    "    \n",
    "    env = gym.make(model)\n",
    "    \n",
    "    obs_len = env.observation_space.shape[0]\n",
    "    obsPre = np.zeros((1,obs_len),dtype=np.float16)\n",
    "    \n",
    "    max_steps = 1000\n",
    "    num_rollouts = 2\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    \n",
    "    for i in range(num_rollouts):\n",
    "        obs = env.reset()\n",
    "        observations.append(obs)\n",
    "        done = False\n",
    "        totalr = 0\n",
    "        steps = 0\n",
    "        \n",
    "        model_in = prepareInput(obs,obsPre,obs_len)\n",
    "        obsPre = obs\n",
    "        preAction = run_policy(model_in)\n",
    "        action = prepareOutput(preAction)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        while True: # Not done\n",
    "            model_in = prepareInput(obs,obsPre,obs_len)\n",
    "            obsPre = obs\n",
    "            preAction = run_policy(model_in)\n",
    "            action = prepareOutput(preAction)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "            if steps % 100 == 0: \n",
    "#                 print(\"\",end=\"\\r\")\n",
    "                print(\"%i/%i\"%(steps, max_steps),end=\"\\r\")\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        returns.append(totalr)\n",
    "        num_rollouts += 1\n",
    "    print('returns', returns)\n",
    "    print('mean return', np.mean(returns))\n",
    "    print('std of return', np.std(returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns [596.6945321141229, 10356.066702258035, 5981.811842800385, 5782.016991939678, 8787.680328430675, 5533.769552097645, 3960.4443004673985, 4161.3833000567765, 5468.963241748297, 6292.278303482952, 4480.91260337196, 4840.124504772794, 8137.004108460847, 7856.549538508789, 8979.386091505408, 10312.568567205644, 5583.106760105452, 3957.674174794266, 1818.6590075460254, 4672.488851214489]\n",
    "\n",
    "mean return 5877.979165144082\n",
    "\n",
    "std of return 2515.448601917892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try now to implement the Dagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (RL)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
